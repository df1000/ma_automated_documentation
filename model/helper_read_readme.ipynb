{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75a29bc",
   "metadata": {},
   "source": [
    "\n",
    "Description: This script provides code to open and read README for a specific GitHub repository from the directory 'data'.  \n",
    "Hint: If lines are created with support of a Large Language Model or the code is taken from another source, you find following hint at the end of the line:\n",
    "      (generated with Microsoft Copilot) or (source: link_to_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23189abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15046454",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/output_evaluation_data_lama_mod/model1/langchain-ai_local-deep-researcher_evaluation_output_mod.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db662d",
   "metadata": {},
   "source": [
    "### Original README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa1bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'r') as f:\n",
    "    loaded_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b61b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Local Deep Researcher\n",
      "\n",
      "Local Deep Researcher is a fully local web research assistant that uses any LLM hosted by [Ollama](https://ollama.com/search) or [LMStudio](https://lmstudio.ai/). Give it a topic and it will generate a web search query, gather web search results, summarize the results of web search, reflect on the summary to examine knowledge gaps, generate a new search query to address the gaps, and repeat for a user-defined number of cycles. It will provide the user a final markdown summary with all sources used to generate the summary.\n",
      "\n",
      "![ollama-deep-research](https://github.com/user-attachments/assets/1c6b28f8-6b64-42ba-a491-1ab2875d50ea)\n",
      "\n",
      "Short summary video:\n",
      "<video src=\"https://github.com/user-attachments/assets/02084902-f067-4658-9683-ff312cab7944\" controls></video>\n",
      "\n",
      "## ðŸ“º Video Tutorials\n",
      "\n",
      "See it in action or build it yourself? Check out these helpful video tutorials:\n",
      "- [Overview of Local Deep Researcher with R1](https://www.youtube.com/watch?v=sGUjmyfof4Q) - Load and test [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) [distilled models](https://ollama.com/library/deepseek-r1).\n",
      "- [Building Local Deep Researcher from Scratch](https://www.youtube.com/watch?v=XGuTzHoqlj8) - Overview of how this is built.\n",
      "\n",
      "## ðŸš€ Quickstart\n",
      "\n",
      "Clone the repository:\n",
      "```shell\n",
      "git clone https://github.com/langchain-ai/local-deep-researcher.git\n",
      "cd local-deep-researcher\n",
      "```\n",
      "\n",
      "Then edit the `.env` file to customize the environment variables according to your needs. These environment variables control the model selection, search tools, and other configuration settings. When you run the application, these values will be automatically loaded via `python-dotenv` (because `langgraph.json` point to the \"env\" file).\n",
      "```shell\n",
      "cp .env.example .env\n",
      "```\n",
      "\n",
      "### Selecting local model with Ollama\n",
      "\n",
      "1. Download the Ollama app for Mac [here](https://ollama.com/download).\n",
      "\n",
      "2. Pull a local LLM from [Ollama](https://ollama.com/search). As an [example](https://ollama.com/library/deepseek-r1:8b):\n",
      "```shell\n",
      "ollama pull deepseek-r1:8b\n",
      "```\n",
      "\n",
      "3. Optionally, update the `.env` file with the following Ollama configuration settings. \n",
      "\n",
      "* If set, these values will take precedence over the defaults set in the `Configuration` class in `configuration.py`. \n",
      "```shell\n",
      "LLM_PROVIDER=ollama\n",
      "OLLAMA_BASE_URL=\"http://localhost:11434\" # Ollama service endpoint, defaults to `http://localhost:11434` \n",
      "LOCAL_LLM=model # the model to use, defaults to `llama3.2` if not set\n",
      "```\n",
      "\n",
      "### Selecting local model with LMStudio\n",
      "\n",
      "1. Download and install LMStudio from [here](https://lmstudio.ai/).\n",
      "\n",
      "2. In LMStudio:\n",
      "   - Download and load your preferred model (e.g., qwen_qwq-32b)\n",
      "   - Go to the \"Local Server\" tab\n",
      "   - Start the server with the OpenAI-compatible API\n",
      "   - Note the server URL (default: http://localhost:1234/v1)\n",
      "\n",
      "3. Optionally, update the `.env` file with the following LMStudio configuration settings. \n",
      "\n",
      "* If set, these values will take precedence over the defaults set in the `Configuration` class in `configuration.py`. \n",
      "```shell\n",
      "LLM_PROVIDER=lmstudio\n",
      "LOCAL_LLM=qwen_qwq-32b  # Use the exact model name as shown in LMStudio\n",
      "LMSTUDIO_BASE_URL=http://localhost:1234/v1\n",
      "```\n",
      "\n",
      "### Selecting search tool\n",
      "\n",
      "By default, it will use [DuckDuckGo](https://duckduckgo.com/) for web search, which does not require an API key. But you can also use [SearXNG](https://docs.searxng.org/), [Tavily](https://tavily.com/) or [Perplexity](https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api) by adding their API keys to the environment file. Optionally, update the `.env` file with the following search tool configuration and API keys. If set, these values will take precedence over the defaults set in the `Configuration` class in `configuration.py`. \n",
      "```shell\n",
      "SEARCH_API=xxx # the search API to use, such as `duckduckgo` (default)\n",
      "TAVILY_API_KEY=xxx # the tavily API key to use\n",
      "PERPLEXITY_API_KEY=xxx # the perplexity API key to use\n",
      "MAX_WEB_RESEARCH_LOOPS=xxx # the maximum number of research loop steps, defaults to `3`\n",
      "FETCH_FULL_PAGE=xxx # fetch the full page content (with `duckduckgo`), defaults to `false`\n",
      "```\n",
      "\n",
      "### Running with LangGraph Studio\n",
      "\n",
      "#### Mac\n",
      "\n",
      "1. (Recommended) Create a virtual environment:\n",
      "```bash\n",
      "python -m venv .venv\n",
      "source .venv/bin/activate\n",
      "```\n",
      "\n",
      "2. Launch LangGraph server:\n",
      "\n",
      "```bash\n",
      "# Install uv package manager\n",
      "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
      "uvx --refresh --from \"langgraph-cli[inmem]\" --with-editable . --python 3.11 langgraph dev\n",
      "```\n",
      "\n",
      "#### Windows\n",
      "\n",
      "1. (Recommended) Create a virtual environment: \n",
      "\n",
      "* Install `Python 3.11` (and add to PATH during installation). \n",
      "* Restart your terminal to ensure Python is available, then create and activate a virtual environment:\n",
      "\n",
      "```powershell\n",
      "python -m venv .venv\n",
      ".venv\\Scripts\\Activate.ps1\n",
      "```\n",
      "\n",
      "2. Launch LangGraph server:\n",
      "\n",
      "```powershell\n",
      "# Install dependencies\n",
      "pip install -e .\n",
      "pip install -U \"langgraph-cli[inmem]\"            \n",
      "\n",
      "# Start the LangGraph server\n",
      "langgraph dev\n",
      "```\n",
      "\n",
      "### Using the LangGraph Studio UI\n",
      "\n",
      "When you launch LangGraph server, you should see the following output and Studio will open in your browser:\n",
      "> Ready!\n",
      "\n",
      "> API: http://127.0.0.1:2024\n",
      "\n",
      "> Docs: http://127.0.0.1:2024/docs\n",
      "\n",
      "> LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
      "\n",
      "Open `LangGraph Studio Web UI` via the URL above. In the `configuration` tab, you can directly set various assistant configurations. Keep in mind that the priority order for configuration values is:\n",
      "\n",
      "```\n",
      "1. Environment variables (highest priority)\n",
      "2. LangGraph UI configuration\n",
      "3. Default values in the Configuration class (lowest priority)\n",
      "```\n",
      "\n",
      "<img width=\"1621\" alt=\"Screenshot 2025-01-24 at 10 08 31 PM\" src=\"https://github.com/user-attachments/assets/7cfd0e04-28fd-4cfa-aee5-9a556d74ab21\" />\n",
      "\n",
      "Give the assistant a topic for research, and you can visualize its process!\n",
      "\n",
      "<img width=\"1621\" alt=\"Screenshot 2025-01-24 at 10 08 22 PM\" src=\"https://github.com/user-attachments/assets/4de6bd89-4f3b-424c-a9cb-70ebd3d45c5f\" />\n",
      "\n",
      "### Model Compatibility Note\n",
      "\n",
      "When selecting a local LLM, set steps use structured JSON output. Some models may have difficulty with this requirement, and the assistant has fallback mechanisms to handle this. As an example, the [DeepSeek R1 (7B)](https://ollama.com/library/deepseek-llm:7b) and [DeepSeek R1 (1.5B)](https://ollama.com/library/deepseek-r1:1.5b) models have difficulty producing required JSON output, and the assistant will use a fallback mechanism to handle this.\n",
      "  \n",
      "### Browser Compatibility Note\n",
      "\n",
      "When accessing the LangGraph Studio UI:\n",
      "- Firefox is recommended for the best experience\n",
      "- Safari users may encounter security warnings due to mixed content (HTTPS/HTTP)\n",
      "- If you encounter issues, try:\n",
      "  1. Using Firefox or another browser\n",
      "  2. Disabling ad-blocking extensions\n",
      "  3. Checking browser console for specific error messages\n",
      "\n",
      "## How it works\n",
      "\n",
      "Local Deep Researcher is inspired by [IterDRAG](https://arxiv.org/html/2410.04343v1#:~:text=To%20tackle%20this%20issue%2C%20we,used%20to%20generate%20intermediate%20answers.). This approach will decompose a query into sub-queries, retrieve documents for each one, answer the sub-query, and then build on the answer by retrieving docs for the second sub-query. Here, we do similar:\n",
      "- Given a user-provided topic, use a local LLM (via [Ollama](https://ollama.com/search) or [LMStudio](https://lmstudio.ai/)) to generate a web search query\n",
      "- Uses a search engine / tool to find relevant sources\n",
      "- Uses LLM to summarize the findings from web search related to the user-provided research topic\n",
      "- Then, it uses the LLM to reflect on the summary, identifying knowledge gaps\n",
      "- It generates a new search query to address the knowledge gaps\n",
      "- The process repeats, with the summary being iteratively updated with new information from web search\n",
      "- Runs for a configurable number of iterations (see `configuration` tab)\n",
      "\n",
      "## Outputs\n",
      "\n",
      "The output of the graph is a markdown file containing the research summary, with citations to the sources used. All sources gathered during research are saved to the graph state. You can visualize them in the graph state, which is visible in LangGraph Studio:\n",
      "\n",
      "![Screenshot 2024-12-05 at 4 08 59 PM](https://github.com/user-attachments/assets/e8ac1c0b-9acb-4a75-8c15-4e677e92f6cb)\n",
      "\n",
      "The final summary is saved to the graph state as well:\n",
      "\n",
      "![Screenshot 2024-12-05 at 4 10 11 PM](https://github.com/user-attachments/assets/f6d997d5-9de5-495f-8556-7d3891f6bc96)\n",
      "\n",
      "## Deployment Options\n",
      "\n",
      "There are [various ways](https://langchain-ai.github.io/langgraph/concepts/#deployment-options) to deploy this graph. See [Module 6](https://github.com/langchain-ai/langchain-academy/tree/main/module-6) of LangChain Academy for a detailed walkthrough of deployment options with LangGraph.\n",
      "\n",
      "## TypeScript Implementation\n",
      "\n",
      "A TypeScript port of this project (without Perplexity search) is available at:\n",
      "https://github.com/PacoVK/ollama-deep-researcher-ts\n",
      "\n",
      "## Running as a Docker container\n",
      "\n",
      "The included `Dockerfile` only runs LangChain Studio with local-deep-researcher as a service, but does not include Ollama as a dependant service. You must run Ollama separately and configure the `OLLAMA_BASE_URL` environment variable. Optionally you can also specify the Ollama model to use by providing the `LOCAL_LLM` environment variable.\n",
      "\n",
      "Clone the repo and build an image:\n",
      "```\n",
      "$ docker build -t local-deep-researcher .\n",
      "```\n",
      "\n",
      "Run the container:\n",
      "```\n",
      "$ docker run --rm -it -p 2024:2024 \\\n",
      "  -e SEARCH_API=\"tavily\" \\ \n",
      "  -e TAVILY_API_KEY=\"tvly-***YOUR_KEY_HERE***\" \\\n",
      "  -e LLM_PROVIDER=ollama\n",
      "  -e OLLAMA_BASE_URL=\"http://host.docker.internal:11434/\" \\\n",
      "  -e LOCAL_LLM=\"llama3.2\" \\  \n",
      "  local-deep-researcher\n",
      "```\n",
      "\n",
      "NOTE: You will see log message:\n",
      "```\n",
      "2025-02-10T13:45:04.784915Z [info     ] ðŸŽ¨ Opening Studio in your browser... [browser_opener] api_variant=local_dev message=ðŸŽ¨ Opening Studio in your browser...\n",
      "URL: https://smith.langchain.com/studio/?baseUrl=http://0.0.0.0:2024\n",
      "```\n",
      "...but the browser will not launch from the container.\n",
      "\n",
      "Instead, visit this link with the correct baseUrl IP address: [`https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024`](https://smith.langchain.com/studio/thread?baseUrl=http://127.0.0.1:2024)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loaded_data['readme_original']['readme'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9ae51",
   "metadata": {},
   "source": [
    "### Generated README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a4b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'r') as f:\n",
    "    loaded_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc4f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Local Deep Researcher\n",
      "\n",
      "## Description\n",
      "\n",
      "The \"local-deep-researcher\" repository is a software development project that implements a research assistant using a graph-based architecture. The purpose of this project is to provide a system that can perform in-depth research on a given topic by iteratively gathering and summarizing information from various web sources.\n",
      "\n",
      "## Installation\n",
      "\n",
      "To get started with the project, you will need to install the required dependencies. The dependencies of this project include:\n",
      "\n",
      "* `langchain-core`\n",
      "* `langchain-ollama`\n",
      "* `langchain-lmstudio`\n",
      "* `tavily`\n",
      "* `duckduckgo-search`\n",
      "* `searxng`\n",
      "* `perplexity-api`\n",
      "\n",
      "You can install these dependencies using pip:\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "## Usage\n",
      "\n",
      "To use the project, you will need to configure the system by creating a configuration file. The configuration file should contain settings for the LLMs, search APIs, and other parameters.\n",
      "\n",
      "Once the configuration is set up, you can run the project using the following command:\n",
      "```bash\n",
      "python main.py\n",
      "```\n",
      "This will start the research process, which will generate a search query, gather sources, summarize the sources, and reflect on the summary to identify knowledge gaps.\n",
      "\n",
      "## Contributing\n",
      "\n",
      "Contributions to the project are welcome! If you would like to contribute, please fork the repository and submit a pull request. Make sure to follow the standard guidelines for contributing to open-source projects.\n",
      "\n",
      "## License\n",
      "\n",
      "The \"local-deep-researcher\" repository is licensed under the MIT License.\n",
      "\n",
      "Copyright (c) 2025 Lance Martin\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n"
     ]
    }
   ],
   "source": [
    "print(loaded_data['readme_genereated']['readme'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
