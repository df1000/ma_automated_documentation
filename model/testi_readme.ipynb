{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c8533da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.cortex import Summarize, Complete, ExtractAnswer, Sentiment, Translate\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d53b07",
   "metadata": {},
   "source": [
    "### Build Snowflake session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3957a221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85edd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = {\n",
    "    \"account\": os.environ['SNOWFLAKE_ACCOUNT'],\n",
    "    \"user\": os.environ['SNOWFLAKE_USER'],\n",
    "    \"password\": os.environ['SNOWFLAKE_USER_PASSWORD'],\n",
    "    \"role\": 'ACCOUNTADMIN',\n",
    "    #\"database\": 'SNOWFLAKE_LEARNING_DB',\n",
    "    \"warehouse\": 'COMPUTE_WH'\n",
    "    #\"schema\": 'PUBLIC',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc98a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_session = Session.builder.configs(connection_params).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d963185",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e80de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'mistral-7b'\n",
    "# max number of input tokens = 32,000\n",
    "# max number of output tokens =  8,192\n",
    "# credits per 1 million token = 0.12 ~ 8 million tokens\n",
    "# model = 'llama3.1-8b'##\n",
    "model = 'llama3.2-3b'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d157ca",
   "metadata": {},
   "source": [
    "### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a4c7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json\n",
    "with open('../data/input_data/kaxap_arl.json', 'r')  as file:\n",
    "    loaded_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edca640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code_cleaned_comments = loaded_data['source_code_cleaned_comments']\n",
    "license = loaded_data['license']\n",
    "requirements = loaded_data['requirements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02bbd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = source_code_cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44b22a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'arl'\n",
    "repo_owner = 'kaxap'\n",
    "license = ''\n",
    "requirements = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc6856",
   "metadata": {},
   "source": [
    "### Create summary prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41adc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = f'''\n",
    "    You are acting like software development expert for the following GitHub repository {repo_name}.\n",
    "    Your task is to summarize the given source code string \"{input_txt}\" in natural language so a specialist is able to understand\n",
    "    the purpose of the repository.\n",
    "    Identify its purpose, key functionalites, main components and dependencies. Focus on the overall architecture and structure \n",
    "    rather than line-by-line details. Do not add any recomandations or improvement suggestions but concentrate on the summary. Present the summary in a clear and concise language. \n",
    "''' \n",
    "prompt_summary = prompt_summary.replace(\"'\", \"\\\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbe6699f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5362"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9649036",
   "metadata": {},
   "source": [
    "### Count input tokens before send to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41b8c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt_input = snowflake_session.sql(f\"SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS('{model}', '{prompt_summary}') AS token_count\").collect()\n",
    "input_tokens = res_cnt_input[0]['TOKEN_COUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b17b91c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c0854",
   "metadata": {},
   "source": [
    "### Execute summary query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b07eecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = snowflake_session.sql(f\"select snowflake.cortex.complete('{model}', '{prompt_summary}') as response\").to_pandas()\n",
    "#query_id = snowflake_session.sql(\"SELECT last_query_id()\").collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3242dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = snowflake_session.sql(f\"select snowflake.cortex.complete('{model}', '{prompt_summary}') as response\").to_pandas()\n",
    "# lama3.2-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c65bdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Repository Summary**\n",
      "\n",
      "The repository is a Python script that generates README files for popular programming languages on GitHub. The script uses the GitHub API to retrieve information about repositories, including their stars, forks, issues, and last commit dates.\n",
      "\n",
      "**Key Functionalities**\n",
      "\n",
      "1. **Language Support**: The script supports a list of programming languages, including Verilog, VHDL, V, Erlang, Kotlin, D, Crystal, Idris, Python, Java, C, CPP, SQL, Node, CSharp, PHP, Ruby, TypeScript, Swift, ObjectiveC, VB.net, Assembly, R, Perl, MATLAB, Go, Scala, Groovy, Lua, Haskell, CoffeeScript, Clojure, Rust, JavaScript, ActionScript, Elixir, Elm, and PureScript.\n",
      "2. **GitHub API Integration**: The script uses the GitHub API to retrieve repository information, including stars, forks, issues, and last commit dates.\n",
      "3. **Rate Limiting**: The script implements rate limiting to prevent excessive requests to the GitHub API, ensuring that the script does not exceed the API's rate limits.\n",
      "4. **Data Processing**: The script processes the retrieved data to generate a table of popular repositories for each language, including their stars, forks, issues, last commit dates, and descriptions.\n",
      "\n",
      "**Main Components**\n",
      "\n",
      "1. **RepositoryInformationProvider**: A class that handles the interaction with the GitHub API, including retrieving repository information and implementing rate limiting.\n",
      "2. **generate_readme**: A function that generates a README file for a given language, using the retrieved repository information.\n",
      "3. **humanize_date**: A function that converts a date string to a human-readable format.\n",
      "\n",
      "**Dependencies**\n",
      "\n",
      "1. **requests**: A Python library for making HTTP requests.\n",
      "2. **json**: A Python library for working with JSON data.\n",
      "3. **humanize**: A Python library for converting dates to human-readable formats.\n",
      "4. **argparse**: A Python library for parsing command-line arguments.\n",
      "\n",
      "**Overall Architecture**\n",
      "\n",
      "The script consists of a single Python file that imports the necessary libraries and defines the main components. The script uses a class-based architecture to organize the code, with the RepositoryInformationProvider class handling the interaction with the GitHub API. The generate_readme function is responsible for generating the README files, while the humanize_date function is used to convert dates to human-readable formats. The script uses a list of supported languages to determine which repositories to retrieve information for.\n"
     ]
    }
   ],
   "source": [
    "summary_txt = df.iloc[0]['RESPONSE']\n",
    "print(summary_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734ae20",
   "metadata": {},
   "source": [
    "### Create README prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d3454351",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_readme = f'''\n",
    "    You are acting like software development expert for the following GitHub repository {repo_name} from the owner {repo_owner}. \n",
    "    Your task is to create a README file for the repository in Markdown format. \n",
    "    Use the provided summary: \"{summary_txt}\", the license: \"{license}\" and the given requirements: \"{requirements}\" \n",
    "    The README file should contain information about what the project does, why it is useful, how users \n",
    "    can get started, where they can get help, and how to maintain and contribute to the project.\n",
    "    If you don't know the answer add a hint following this style […]. You're not allowed to create \n",
    "    made-up content to fill gaps and you're not allowed to add additional paragraphs.\n",
    "    Use the following Markdown template and fill each paragraph. \n",
    "\n",
    "    ## Titel\n",
    "\n",
    "    ## Installation\n",
    "\n",
    "    ## Usage\n",
    "\n",
    "    ## Contributing\n",
    "\n",
    "    ## License\n",
    "\n",
    "    Do not include any sensitive data like names or emails. Keep the markdown file clean and structured.\n",
    "'''\n",
    "prompt_readme = prompt_readme.replace(\"'\", \"\\\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "17e82682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = snowflake_session.sql(f\"select snowflake.cortex.complete('{model}', '{prompt_readme}') as response\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cb7b7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_readme = df.iloc[0]['RESPONSE']\n",
    "readme_txt = df_readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5bfa2019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ## Popular GitHub Repositories by Programming Language\n",
      "\n",
      "This repository is a Python script that fetches and generates a list of most popular repositories on GitHub based on the given programming language. It uses the GitHub API to retrieve the required information and stores the access token in a local file named \"token.json\". The script supports multiple programming languages and can fetch up to 10 pages of results per language.\n",
      "\n",
      "## Installation\n",
      "\n",
      "To use this script, you need to have Python installed on your system. You can install the required dependencies using pip:\n",
      "\n",
      "```bash\n",
      "pip install requests argparse json humanize\n",
      "```\n",
      "\n",
      "## Usage\n",
      "\n",
      "To run the script, save the provided code in a file named `github_popular_repos.py` and execute it using the following command:\n",
      "\n",
      "```bash\n",
      "python github_popular_repos.py [--language LANG1, LANG2, ...]\n",
      "```\n",
      "\n",
      "Replace `LANG1, LANG2, ...` with the desired programming languages, separated by commas. If no languages are specified, the script will fetch the popular repositories for all supported languages.\n",
      "\n",
      "The script generates a markdown file named `repos.md` in the same directory with the most popular repositories for the given languages.\n",
      "\n",
      "## Contributing\n",
      "\n",
      "Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.\n",
      "\n",
      "## License\n",
      "\n",
      "This project is licensed under the [MIT License](LICENSE).\n",
      "\n",
      "[MIT License](LICENSE)\n",
      "\n",
      "## Development\n",
      "\n",
      "To run the script in development mode, you can use the following command:\n",
      "\n",
      "```bash\n",
      "python -m unittest tests.test_github_popular_repos.py\n",
      "```\n",
      "\n",
      "This will run the unit tests for the script. The tests cover the main functionality of the script and ensure that the expected results are generated.\n",
      "\n",
      "## Dependencies\n",
      "\n",
      "The script depends on the following Python libraries:\n",
      "\n",
      "- `requests`: for making HTTP requests\n",
      "- `argparse`: for parsing command-line arguments\n",
      "- `json`: for parsing JSON responses\n",
      "- `time`: for handling time-related functionality\n",
      "- `humanize`: for formatting dates\n",
      "- `datetime`: for parsing and manipulating dates\n",
      "\n",
      "You can install these dependencies using pip:\n",
      "\n",
      "```bash\n",
      "pip install requests argparse json humanize\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(df_readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bc15a",
   "metadata": {},
   "source": [
    "### Save summary and README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b07c1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_json = {\n",
    "    'repo_owner': repo_owner,\n",
    "    'repo_name': repo_name,\n",
    "    'summary': summary_txt,\n",
    "    'readme': readme_txt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "57cda829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_owner': 'kaxap',\n",
       " 'repo_name': 'arl',\n",
       " 'summary': ' This repository is a Python script that fetches and generates a list of most popular repositories on GitHub based on the given programming language. The script uses the GitHub API to retrieve the required information and stores the access token in a local file named \"token.json\". The script supports multiple programming languages and can fetch up to 10 pages of results per language.\\n\\nThe script defines a class `RepositoryInformationProvider` that initializes a `requests.Session` object with retries and rate limit handling. It also defines methods to get the next page of results for a given language and to get the last commit date for a given repository.\\n\\nThe `generate_readme` function generates a markdown file with a table of most popular repositories for a given language. It fetches the data using the `RepositoryInformationProvider` and formats the data into a markdown table.\\n\\nThe script uses several constants and variables to store the API URLs, headers, and other configuration options. It also defines some helper functions for formatting and humanizing dates.\\n\\nThe script uses the `argparse` module to parse command-line arguments and supports specifying multiple languages using a comma-separated list.\\n\\nThe main components of the script are:\\n\\n* `RepositoryInformationProvider` class for fetching and handling GitHub API responses\\n* `generate_readme` function for generating the markdown file\\n* Use of `requests` library for making HTTP requests\\n* Use of `argparse` module for parsing command-line arguments\\n* Use of `json`, `time`, `humanize`, `datetime`, and `argparse` modules for various utility functions\\n\\nThe dependencies of the script are:\\n\\n* `requests` library for making HTTP requests\\n* `argparse` module for parsing command-line arguments\\n* `json` module for parsing JSON responses\\n* `time` module for handling time-related functionality\\n* `humanize` module for formatting dates\\n* `datetime` module for parsing and manipulating dates\\n\\nThe overall architecture of the script is simple and modular, with clear separation of concerns between fetching data from the API and generating the markdown file. The script is well-documented with clear variable and function names, making it easy to understand and maintain.',\n",
       " 'readme': ' ## Popular GitHub Repositories by Programming Language\\n\\nThis repository is a Python script that fetches and generates a list of most popular repositories on GitHub based on the given programming language. It uses the GitHub API to retrieve the required information and stores the access token in a local file named \"token.json\". The script supports multiple programming languages and can fetch up to 10 pages of results per language.\\n\\n## Installation\\n\\nTo use this script, you need to have Python installed on your system. You can install the required dependencies using pip:\\n\\n```bash\\npip install requests argparse json humanize\\n```\\n\\n## Usage\\n\\nTo run the script, save the provided code in a file named `github_popular_repos.py` and execute it using the following command:\\n\\n```bash\\npython github_popular_repos.py [--language LANG1, LANG2, ...]\\n```\\n\\nReplace `LANG1, LANG2, ...` with the desired programming languages, separated by commas. If no languages are specified, the script will fetch the popular repositories for all supported languages.\\n\\nThe script generates a markdown file named `repos.md` in the same directory with the most popular repositories for the given languages.\\n\\n## Contributing\\n\\nContributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.\\n\\n## License\\n\\nThis project is licensed under the [MIT License](LICENSE).\\n\\n[MIT License](LICENSE)\\n\\n## Development\\n\\nTo run the script in development mode, you can use the following command:\\n\\n```bash\\npython -m unittest tests.test_github_popular_repos.py\\n```\\n\\nThis will run the unit tests for the script. The tests cover the main functionality of the script and ensure that the expected results are generated.\\n\\n## Dependencies\\n\\nThe script depends on the following Python libraries:\\n\\n- `requests`: for making HTTP requests\\n- `argparse`: for parsing command-line arguments\\n- `json`: for parsing JSON responses\\n- `time`: for handling time-related functionality\\n- `humanize`: for formatting dates\\n- `datetime`: for parsing and manipulating dates\\n\\nYou can install these dependencies using pip:\\n\\n```bash\\npip install requests argparse json humanize\\n```'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d1a3b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../model/test_readme.json', 'w') as file:\n",
    "    json.dump(tmp_json, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f41557",
   "metadata": {},
   "source": [
    "### Count output tokens from prompt and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bfd9ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = snowflake_session.sql(\"SELECT last_query_id()\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f3ca7300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01bc4ca8-0305-3f74-0008-537300054062\n"
     ]
    }
   ],
   "source": [
    "print(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3091c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt_output = snowflake_session.sql(f\"SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY WHERE query_id='{query_id}'\").to_pandas()\n",
    "output_tokens = res_cnt_output['TOKENS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eda5084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QUERY_ID</th>\n",
       "      <th>WAREHOUSE_ID</th>\n",
       "      <th>MODEL_NAME</th>\n",
       "      <th>FUNCTION_NAME</th>\n",
       "      <th>TOKENS</th>\n",
       "      <th>TOKEN_CREDITS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [QUERY_ID, WAREHOUSE_ID, MODEL_NAME, FUNCTION_NAME, TOKENS, TOKEN_CREDITS]\n",
       "Index: []"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_cnt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e80b09cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n",
      "Series([], Name: TOKENS, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens)\n",
    "print(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9b37ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count input tokens of prompt\n",
    "\n",
    "\n",
    "# # Get current Snowflake session\n",
    "# #session = get_active_session()\n",
    "\n",
    "# # Define model and input text\n",
    "# model_name = model #0.06 credits per 1M token\n",
    "\n",
    "# # Execute token count function\n",
    "# result = session.sql(f\"SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS('{model_name}', '{prompt_summary}') AS token_count\").collect()\n",
    "\n",
    "# print(f\"Token count: {result[0]['TOKEN_COUNT']}\") # 204 tokens (snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6bedd0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get query id of last session\n",
    "# query_id = session.sql(\"SELECT last_query_id()\").collect()[0][0]\n",
    "# print(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "07b7e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_id = '01bc4c11-0305-3f6c-0008-53730004c00e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9d1b5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = get_active_session()\n",
    "# get tokens for query id\n",
    "#res = snowflake_session.sql(f\"SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY WHERE query_id='{query_id}'\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4dbbac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tokens = result[0]['TOKEN_COUNT']\n",
    "# output_tokens = res['TOKENS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1eebdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daea7f7",
   "metadata": {},
   "source": [
    "Interessant wird das repo tensorflow models! Es hat die meiste Anzahl an Characters ;-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
